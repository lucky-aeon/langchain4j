package dev.langchain4j.model.chat.response;

import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.model.chat.StreamingChatModel;

/**
 * Represents a handler for a {@link StreamingChatModel} response.
 *
 * @see StreamingChatModel
 */
public interface StreamingChatResponseHandler {

    /**
     * Invoked each time the model generates a partial response (usually a single token) in a textual response.
     * If the model decides to execute a tool instead, this method will not be invoked;
     * {@link #onCompleteResponse} will be invoked instead.
     *
     * @param partialResponse The partial response (usually a single token), which is a part of the complete response.
     */
    void onPartialResponse(String partialResponse);

    /**
     * Invoked each time the model generates a partial reasoning/thinking content.
     * This is called for models that support reasoning chains (like OpenAI o1 series).
     *
     * @param partialReasoning The partial reasoning content (usually a single token), which is a part of the complete reasoning.
     */
    default void onPartialReasoning(String partialReasoning) {
        // Default implementation does nothing
    }

    /**
     * Invoked when the model has finished streaming a response.
     * If the model requests the execution of one or multiple tools,
     * this can be accessed via {@link ChatResponse#aiMessage()} -> {@link AiMessage#toolExecutionRequests()}.
     *
     * @param completeResponse The complete response generated by the model.
     *                         For textual responses, it contains all tokens from {@link #onPartialResponse} concatenated.
     */
    void onCompleteResponse(ChatResponse completeResponse);

    /**
     * Invoked when the model has finished generating reasoning content.
     * This contains the complete reasoning/thinking process for models that support it.
     *
     * @param completeReasoning The complete reasoning content generated by the model.
     */
    default void onCompleteReasoning(String completeReasoning) {
        // Default implementation does nothing
    }

    /**
     * Invoked with raw data from the streaming response for custom processing.
     * This method receives the original raw data before any processing, 
     * allowing for custom reasoning detection and content extraction.
     * 
     * This is particularly useful for models that embed reasoning content
     * in the raw streaming data that needs to be detected and separated
     * from regular response content.
     *
     * @param rawData The raw data chunk from the streaming response
     */
    default void onRawData(Object rawData) {
        // Default implementation does nothing
    }

    /**
     * This method is invoked when an error occurs during streaming.
     *
     * @param error The error that occurred
     */
    void onError(Throwable error);
}
